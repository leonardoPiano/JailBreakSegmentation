{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c89170b1-3bd1-4643-be41-7c0cb752eb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unica/anaconda3/envs/vLLM_leo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-16 16:04:02 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 16:04:04,027\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/home/unica/anaconda3/envs/vLLM_leo/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:902: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "HF_TOKEN=os.environ.get(\"HF_TOKEN\")\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"HUGGINFACEHUB_API_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "from transformers import AutoConfig, AutoTokenizer,AutoModelForCausalLM\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "model_id = \"meta-llama/Llama-Guard-3-8B\"\n",
    "dtype = torch.bfloat16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c27a7a13-1873-4e1e-8f20-d154b7eebfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-16 16:04:11 [config.py:717] This model supports multiple tasks: {'generate', 'embed', 'classify', 'score', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 06-16 16:04:11 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 06-16 16:04:13 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='meta-llama/Llama-Guard-3-8B', speculative_config=None, tokenizer='meta-llama/Llama-Guard-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-Guard-3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 06-16 16:04:13 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f48b52a4c90>\n",
      "INFO 06-16 16:04:30 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 06-16 16:04:30 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 06-16 16:04:30 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 06-16 16:04:30 [gpu_model_runner.py:1329] Starting to load model meta-llama/Llama-Guard-3-8B...\n",
      "INFO 06-16 16:04:30 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.74it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.78s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:05<00:02,  2.15s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08<00:00,  2.28s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08<00:00,  2.07s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-16 16:04:39 [loader.py:458] Loading weights took 8.44 seconds\n",
      "INFO 06-16 16:04:39 [gpu_model_runner.py:1347] Model loading took 14.9889 GiB and 9.243428 seconds\n",
      "INFO 06-16 16:04:44 [backends.py:420] Using cache directory: /home/unica/.cache/vllm/torch_compile_cache/76bc7ce0f4/rank_0_0 for vLLM's torch.compile\n",
      "INFO 06-16 16:04:44 [backends.py:430] Dynamo bytecode transform time: 5.45 s\n",
      "INFO 06-16 16:04:48 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 3.035 s\n",
      "INFO 06-16 16:04:49 [monitor.py:33] torch.compile takes 5.45 s in total\n",
      "INFO 06-16 16:04:50 [kv_cache_utils.py:634] GPU KV cache size: 209,216 tokens\n",
      "INFO 06-16 16:04:50 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 1.60x\n",
      "INFO 06-16 16:05:12 [gpu_model_runner.py:1686] Graph capturing finished in 22 secs, took 0.51 GiB\n",
      "INFO 06-16 16:05:12 [core.py:159] init engine (profile, create kv cache, warmup model) took 32.99 seconds\n",
      "INFO 06-16 16:05:12 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "llm =LLM(model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "534154c7-49ee-4d56-929c-6590c8ff54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "sap_200_categories = os.listdir(\"../Jailbreak_attacks/attacks2test/SAP200/\")\n",
    "sap_prompts=[]\n",
    "for category in sap_200_categories:\n",
    "    attack = json.load(open(f\"../Jailbreak_attacks/attacks2test/SAP200/{category}/generated_cases.json\"))\n",
    "    for item in attack:\n",
    "        sap_prompts.append({\"category\": category, \"prompt\": item[\"Attack Prompt\"]})\n",
    "        \n",
    "deep_inception=json.load(open(\"../Jailbreak_attacks/attacks2test/DeepInception/data.json\",\"r\"))\n",
    "dan=json.load(open(\"../Jailbreak_attacks/attacks2test/DAN/Dan_samples.json\"))\n",
    "deep_prompts=[{\"prompt\":item[\"inception_attack\"],\"ann\":[item[\"plain_attack\"].lower().replace(\".\",\"\"),1]} for item in deep_inception]\n",
    "dan_prompts=[{\"prompt\":item[\"full_prompt\"],\"ann\":[item[\"question\"],1]} for item in dan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "396faf0c-5319-4ca8-b019-944351469444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DAN\n",
      "INFO 06-11 13:26:52 [chat_utils.py:397] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████| 1560/1560 [00:20<00:00, 76.95it/s, est. speed input: 59457.12 toks/s, output: 375.15 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SAP200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████| 1600/1600 [00:46<00:00, 34.51it/s, est. speed input: 12102.27 toks/s, output: 200.70 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DeepInception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████████| 50/50 [00:00<00:00, 50.98it/s, est. speed input: 15988.95 toks/s, output: 293.69 toks/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.00, top_p=1, max_tokens=25)\n",
    "attacks={\"DAN\":dan_prompts,\"SAP200\":sap_prompts,\"DeepInception\":deep_prompts}\n",
    "for attack,data in attacks.items():\n",
    "    json_output=[]    \n",
    "    print(\"Processing\",attack)\n",
    "    messages=[[{\"role\": \"user\", \"content\": x[\"prompt\"]}] for x in data]\n",
    "    outputs = llm.chat(messages, sampling_params)\n",
    "    for i, output in enumerate(outputs):\n",
    "        generated_text = output.outputs[0].text\n",
    "        prompt = data[i][\"prompt\"]\n",
    "        json_output.append({\"prompt\": prompt, \"label\": generated_text})\n",
    "    with open(f\"output/{attack}_llamaguard3.json\",\"w\") as f:\n",
    "        json.dump(json_output,f,indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
